{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import DQN, ReplayBuffer, greedy_action, epsilon_greedy, update_target, loss\n",
    "\n",
    "import gym\n",
    "from gym import Env\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.optim as optim\n",
    "from torch.optim import Adam, RAdam, SGD\n",
    "import os\n",
    "from collections import namedtuple\n",
    "from collections import defaultdictSS\n",
    "import matplotlib.patches as mpatches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing Hyperparameters against Convergence and Average Reward Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a named tuple to store transition details in the replay buffer\n",
    "Transition = namedtuple('Transition', ('state', 'action', 'next_state', 'reward', 'done'))\n",
    "\n",
    "# Initialize the CartPole environment from the OpenAI Gym\n",
    "env = gym.make('CartPole-v1')\n",
    "\n",
    "def has_converged(rewards, threshold, window_size):\n",
    "    \"\"\"\n",
    "    Checks if the learning has converged based on a moving average of rewards.\n",
    "\n",
    "    Args:\n",
    "    rewards (list): List of accumulated rewards per episode.\n",
    "    threshold (float): The threshold value for the moving average to exceed for convergence.\n",
    "    window_size (int): The number of episodes to consider for the moving average.\n",
    "\n",
    "    Returns:\n",
    "    bool: True if the average reward over the last `window_size` episodes exceeds `threshold`.\n",
    "    \"\"\"\n",
    "    if len(rewards) >= window_size:\n",
    "        return np.mean(rewards[-window_size:]) > threshold\n",
    "    return False\n",
    "\n",
    "# Define hyperparameters\n",
    "NUM_RUNS = 10\n",
    "NUM_EPISODES = 100\n",
    "lr = [0.001, 0.01, 0.1]\n",
    "buffer_range = [100,500,1000,5000,10000,20000]\n",
    "e_range = [1,0.99,0.9,0]\n",
    "e_decays = [0.1, 0.2,0.3,0.4, 0.5,0.6, 0.7 ,0.8,0.9 ,1]\n",
    "policy_nn_range = [[4,32,16,2],[4,128,64,32,2],[4, 128, 64, 2],[4, 16, 2],[4, 32, 2],[4,64,2], [4, 128, 2]]\n",
    "batch_sizes = [1, 5, 10, 20, 32]\n",
    "convergence_threshold = 50\n",
    "convergence_window = 10\n",
    "optimisers = ['SGD','Adam','Radam','RMSprop']\n",
    "act_funcs = [F.tanh, F.relu, F.leaky_relu, F.sigmoid]\n",
    "iteration_range = [1,2,3,4,5,10,20]\n",
    "\n",
    "# Initialize variables to track the best hyperparameter configuration\n",
    "best_convergence_rate = float('inf')\n",
    "best_average_reward = 0\n",
    "best_hyperparameters = None\n",
    "\n",
    "# Array to store results of different runs\n",
    "runs_results = []\n",
    "\n",
    "# Nested loops for hyperparameter tuning\n",
    "for lr in lr:\n",
    "    for buffer_size in buffer_range:\n",
    "        for epsilon in e_range:\n",
    "            for epsilon_decay in e_decays:\n",
    "                for nn_architecture in policy_nn_range:\n",
    "                    for batch_size in batch_sizes:\n",
    "                        config_results = []\n",
    "\n",
    "                        for run in range(NUM_RUNS):\n",
    "                            # Initialize the policy network, target network, optimizer, and replay buffer for each run\n",
    "                            policy_net = DQN(nn_architecture, F.relu)\n",
    "                            target_net = DQN(nn_architecture, F.relu)\n",
    "                            update_target(target_net, policy_net)\n",
    "                            optimizer = optim.Adam(policy_net.parameters(), lr=lr)\n",
    "                            memory = ReplayBuffer(buffer_size)\n",
    "\n",
    "                            steps_done = epsilon\n",
    "                            episode_rewards = []\n",
    "                            converged_in_episode = None\n",
    "\n",
    "                            for i_episode in range(NUM_EPISODES):\n",
    "                                # Environment reset and initial state preparation\n",
    "                                observation, info = env.reset()\n",
    "                                state = torch.tensor(observation).float()\n",
    "                                current_episode_reward = 0\n",
    "                                done = False\n",
    "\n",
    "                                while not done:\n",
    "                                    # Select and perform an action using epsilon-greedy policy\n",
    "                                    action = epsilon_greedy(steps_done, policy_net, state)\n",
    "                                    response = env.step(action)\n",
    "\n",
    "                                    # Handling different formats of responses from the environment\n",
    "                                    if len(response) == 5:\n",
    "                                        next_observation, reward, done, extra_boolean, info = response\n",
    "                                    else:\n",
    "                                        raise ValueError(f\"Unexpected response format from env.step: {response}\")\n",
    "\n",
    "                                    # Update state and store the transition in the replay buffer\n",
    "                                    next_state = torch.tensor(next_observation).float()\n",
    "                                    reward_tensor = torch.tensor([reward])\n",
    "                                    memory.push((state, torch.tensor([action]), next_state, reward_tensor, torch.tensor([done])))\n",
    "\n",
    "                                    state = next_state\n",
    "                                    current_episode_reward += reward\n",
    "\n",
    "                                    # Training the model with a batch of transitions from the replay buffer\n",
    "                                    if len(memory.buffer) >= batch_size:\n",
    "                                        transitions = memory.sample(batch_size)\n",
    "                                        batch = Transition(*zip(*transitions))\n",
    "                                        # Extract and reshape tensors from the batch\n",
    "                                        state_batch = torch.cat(batch.state).view(batch_size, -1)\n",
    "                                        action_batch = torch.cat(batch.action).view(batch_size, -1)\n",
    "                                        reward_batch = torch.cat(batch.reward).view(batch_size, -1)\n",
    "                                        next_state_batch = torch.cat(batch.next_state).view(batch_size, -1)\n",
    "                                        done_batch = torch.cat(batch.done).view(batch_size, -1)\n",
    "\n",
    "                                        mse_loss = loss(policy_net, target_net, state_batch, action_batch, reward_batch, next_state_batch, done_batch)\n",
    "                                        optimizer.zero_grad()\n",
    "                                        mse_loss.backward()\n",
    "                                        optimizer.step()\n",
    "\n",
    "                                    # Decay epsilon after each action\n",
    "                                    steps_done *= epsilon_decay\n",
    "\n",
    "                                episode_rewards.append(current_episode_reward)\n",
    "                                if converged_in_episode is None and has_converged(episode_rewards, convergence_threshold, convergence_window):\n",
    "                                    converged_in_episode = i_episode\n",
    "\n",
    "                            config_results.append(episode_rewards)\n",
    "                        \n",
    "                            runs_results.append(config_results)\n",
    "\n",
    "                            # Update best hyperparameters if current configuration is better\n",
    "                            if converged_in_episode is not None and len(episode_rewards) > converged_in_episode:\n",
    "                                average_reward_post_convergence = np.mean(episode_rewards[converged_in_episode:])\n",
    "                                convergence_rate = converged_in_episode\n",
    "\n",
    "                                if convergence_rate < best_convergence_rate or (convergence_rate == best_convergence_rate and average_reward_post_convergence > best_average_reward):\n",
    "                                    best_convergence_rate = convergence_rate\n",
    "                                    best_average_reward = average_reward_post_convergence\n",
    "                                    best_hyperparameters = {\n",
    "                                        'lr': lr,\n",
    "                                        'buffer_size': buffer_size,\n",
    "                                        'epsilon': epsilon,\n",
    "                                        'epsilon_decay': epsilon_decay,\n",
    "                                        'nn_architecture': nn_architecture,\n",
    "                                        'batch_size': batch_size\n",
    "                                    }        \n",
    "                        env.close()\n",
    "\n",
    "print(\"Best Hyperparameters based on Convergence Rate and Average Reward Post-Convergence:\")\n",
    "print(best_hyperparameters)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_optimizer(policy_network, lr, optimiser_name):\n",
    "    \"\"\"\n",
    "    Selects and initializes an optimizer for the policy network based on the given name.\n",
    "\n",
    "    Args:\n",
    "    policy_network (nn.Module): The neural network model for which the optimizer will be used.\n",
    "    lr (float): Learning rate for the optimizer.\n",
    "    optimiser_name (str): Name of the optimizer to be used.\n",
    "\n",
    "    Returns:\n",
    "    torch.optim.Optimizer: An initialized optimizer.\n",
    "\n",
    "    Raises:\n",
    "    ValueError: If the optimizer name is not recognized.\n",
    "    \"\"\"\n",
    "    optimizers = {\n",
    "        'Adam': optim.Adam,\n",
    "        'RAdam': optim.RAdam,\n",
    "        'SGD': optim.SGD,\n",
    "        'RMSprop' : optim.RMSprop \n",
    "    }\n",
    "    if optimiser_name in optimizers:\n",
    "        return optimizers[optimiser_name](policy_network.parameters(), lr=lr)\n",
    "    else:\n",
    "        raise ValueError(\"Invalid optimizer\")\n",
    "\n",
    "def initialize_environment():\n",
    "    \"\"\"\n",
    "    Initializes and returns the CartPole-v1 environment from OpenAI Gym.\n",
    "\n",
    "    Returns:\n",
    "    gym.Env: The initialized environment.\n",
    "    \"\"\"\n",
    "    return gym.make('CartPole-v1')\n",
    "\n",
    "def initialize_episode(env):\n",
    "    \"\"\"\n",
    "    Initializes a new episode in the given environment.\n",
    "\n",
    "    Args:\n",
    "    env (gym.Env): The environment in which the episode is to be initialized.\n",
    "\n",
    "    Returns:\n",
    "    torch.Tensor: The initial state of the environment as a tensor.\n",
    "    \"\"\"\n",
    "    observation, _ = env.reset()\n",
    "    state = torch.tensor(observation).float()\n",
    "    return state\n",
    "\n",
    "def perform_episode_step(env, policy_network, state, memory, e):\n",
    "    \"\"\"\n",
    "    Performs a single step in an episode, including action selection, environment interaction, and memory update.\n",
    "\n",
    "    Args:\n",
    "    env (gym.Env): The environment in which the episode is occurring.\n",
    "    policy_network (nn.Module): The policy network used for action selection.\n",
    "    state (torch.Tensor): The current state of the environment.\n",
    "    memory (ReplayBuffer): The memory buffer to store experiences.\n",
    "    e (float): The epsilon value for epsilon-greedy action selection.\n",
    "\n",
    "    Returns:\n",
    "    tuple: The next state, and boolean flags indicating if the episode is done or terminated.\n",
    "    \"\"\"\n",
    "    action = epsilon_greedy(e, policy_network, state)\n",
    "    observation, reward, done, terminated, _ = env.step(action)\n",
    "    reward = torch.tensor([reward])\n",
    "    action = torch.tensor([action])\n",
    "    next_state = torch.tensor(observation).reshape(-1).float()\n",
    "    memory.push([state, action, next_state, reward, torch.tensor([done])])\n",
    "    return next_state, done, terminated\n",
    "\n",
    "def batch(policy_network, target_net, optimizer, memory, batch_space):\n",
    "    \"\"\"\n",
    "    Processes a batch of experiences to train the policy network.\n",
    "\n",
    "    Args:\n",
    "    policy_network (nn.Module): The policy network to be trained.\n",
    "    target_net (nn.Module): The target network used for stable Q-value estimation.\n",
    "    optimizer (torch.optim.Optimizer): The optimizer used for training.\n",
    "    memory (ReplayBuffer): The replay buffer containing experiences.\n",
    "    batch_space (int): The size of the batch to be used for training.\n",
    "    \"\"\"\n",
    "    if len(memory.buffer) < batch_space:\n",
    "        return\n",
    "\n",
    "    transitions = memory.sample(batch_space)\n",
    "    state_batch, action_batch, nextstate_batch, reward_batch, dones = (torch.stack(x) for x in zip(*transitions))\n",
    "    mse_loss = loss(policy_network, target_net, state_batch, action_batch, reward_batch, nextstate_batch, dones)\n",
    "    optimizer.zero_grad()\n",
    "    mse_loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "def single_run(env, policy_network, target_net, optimizer, memory, episodes, batch_space, e, e_decay, update_target_iteration):\n",
    "    \"\"\"\n",
    "    Executes a single run of the training process.\n",
    "\n",
    "    Args:\n",
    "    env (gym.Env): The environment to train in.\n",
    "    policy_network (nn.Module): The policy network.\n",
    "    target_net (nn.Module): The target network.\n",
    "    optimizer (torch.optim.Optimizer): The optimizer for training.\n",
    "    memory (ReplayBuffer): The replay buffer.\n",
    "    episodes (int): The number of episodes to run.\n",
    "    batch_space (int): The batch size for training.\n",
    "    e (float): Initial epsilon value for epsilon-greedy strategy.\n",
    "    e_decay (float): Decay rate for epsilon.\n",
    "    update_target_iteration (int): Number of episodes after which the target network is updated.\n",
    "\n",
    "    Returns:\n",
    "    list: A list of episode durations for the run.\n",
    "    \"\"\"\n",
    "    episode_durations = []\n",
    "\n",
    "    for i_episode in range(episodes):\n",
    "        state = initialize_episode(env)\n",
    "        done = False\n",
    "        terminated = False\n",
    "        t = 0\n",
    "\n",
    "        while not (done or terminated):\n",
    "            state, done, terminated = perform_episode_step(env, policy_network, state, memory, e)\n",
    "            batch(policy_network, target_net, optimizer, memory, batch_space)\n",
    "            \n",
    "            if done or terminated:\n",
    "                episode_durations.append(t + 1)\n",
    "            t += 1\n",
    "\n",
    "            e = max(0.01, e * e_decay)\n",
    "\n",
    "        if i_episode % update_target_iteration == 0: \n",
    "            update_target(target_net, policy_network)\n",
    "\n",
    "    return episode_durations\n",
    "\n",
    "def run_dqn(e, e_decay, lr, optimiser, policy_nn, act_func, batch_space, memory_replay_buffer_size, update_target_iteration, episodes, num_runs=10):\n",
    "    \"\"\"\n",
    "    Executes the DQN training process over a specified number of runs.\n",
    "\n",
    "    Args:\n",
    "    e (float): Initial epsilon value for epsilon-greedy strategy.\n",
    "    e_decay (float): Decay rate for epsilon.\n",
    "    lr (float): Learning rate for the optimizer.\n",
    "    optimiser (str): Name of the optimizer to be used.\n",
    "    policy_nn (list): Neural network architecture for the policy network.\n",
    "    act_func (function): Activation function to be used in the policy network.\n",
    "    batch_space (int): The batch size for training.\n",
    "    memory_replay_buffer_size (int): The size of the replay buffer.\n",
    "    update_target_iteration (int): Number of episodes after which the target network is updated.\n",
    "    episodes (int): The number of episodes per run.\n",
    "    num_runs (int, optional): The number of training runs to perform. Defaults to 1.\n",
    "\n",
    "    Returns:\n",
    "    list: A list containing the episode durations for each run.\n",
    "    \"\"\"\n",
    "    runs_results = []\n",
    "    env = initialize_environment()\n",
    "\n",
    "    for _ in range(num_runs):\n",
    "        policy_network = DQN(policy_nn, act_func)\n",
    "        target_net = DQN(policy_nn, act_func)\n",
    "        update_target(target_net, policy_network)\n",
    "        target_net.eval()\n",
    "\n",
    "        optimizer = select_optimizer(policy_network, lr, optimiser)\n",
    "\n",
    "        memory = ReplayBuffer(memory_replay_buffer_size)\n",
    "\n",
    "        episode_durations = single_run(\n",
    "            env, policy_network, target_net, optimizer, memory, episodes, batch_space, e, e_decay, update_target_iteration\n",
    "        )\n",
    "        runs_results.append(episode_durations)\n",
    "\n",
    "    return runs_results\n",
    "\n",
    "\n",
    "# Define hyperparameters as individual variables\n",
    "# Define hyperparameters for the DQN algorithm\n",
    "e = 0.8  # Initial epsilon value for epsilon-greedy action selection\n",
    "e_decay = 0.99  # Epsilon decay rate after each episode\n",
    "update_target_iteration = 1  # Number of episodes after which to update the target network\n",
    "lr = 0.001  # Learning rate for the optimizer\n",
    "act_func = F.leaky_relu  # Activation function to be used in the neural network\n",
    "batch_space = 32  # Batch size for training\n",
    "memory_replay_buffer_size = 2000  # Size of the replay buffer\n",
    "optimiser = 'Adam'  # Name of the optimizer to be used\n",
    "policy_nn = [4, 128, 64, 2]  # Neural network architecture for the policy network\n",
    "\n",
    "episodes = 300  # Number of episodes to run the DQN algorithm\n",
    "\n",
    "# Run the DQN algorithm with the defined hyperparameters and print the results\n",
    "# runs_results = run_dqn(\n",
    "#     e, e_decay, lr, optimiser, policy_nn, act_func,\n",
    "#     batch_space, memory_replay_buffer_size, update_target_iteration, episodes\n",
    "# )\n",
    "\n",
    "# print(runs_results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_tensor = torch.tensor(runs_results)\n",
    "\n",
    "means = results_tensor.float().mean(dim=0)\n",
    "stds = results_tensor.float().std(dim=0)\n",
    "\n",
    "episodes_value = 300\n",
    "# Plotting\n",
    "episodes = torch.arange(1, episodes_value + 1)  # Adjusted to match the number of episodes\n",
    "plt.plot(episodes, means, label='Mean Duration')\n",
    "plt.fill_between(episodes, means - stds, means + stds, alpha=0.3, color='blue', label='Std. Deviation')\n",
    "\n",
    "plt.axhline(y=100, color='red', linestyle='--', label='Target Duration')\n",
    "plt.title('Training Performance Over Episodes')\n",
    "plt.ylabel(\"Average Episode Duration\")\n",
    "plt.xlabel(\"Episode\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "NUM_EPISODES = 250\n",
    "EPISODE_RANGE = torch.arange(NUM_EPISODES)\n",
    "BLUE_LINE_Y = 100\n",
    "BLUE_LINE_COLOR = 'blue'\n",
    "BLUE_LINE_STYLE = '--'\n",
    "FIG_SIZE = (20, 10)\n",
    "FONT_SIZE = 18\n",
    "LEGEND_LOC = 'upper left'\n",
    "LINE_WIDTH = 2\n",
    "GRID_STYLE = 'dashed'\n",
    "COLOR_PALETTE = plt.cm.viridis  # Using a color palette for lines\n",
    "\n",
    "def parameter_testing(baseline, parameter, values):\n",
    "    results = []\n",
    "    print(f\"Testing parameter: {parameter}\")\n",
    "\n",
    "    for value in values:\n",
    "        configuration = {**baseline, parameter: value}\n",
    "        print(f\"Parameter value: {value}\\n{configuration}\")\n",
    "\n",
    "        run_results = run_dqn(num_runs=1, **configuration)\n",
    "        results.append(run_results)\n",
    "\n",
    "    return results\n",
    "\n",
    "def param_plot(range_of_results, parameters, title, save_path='images'):\n",
    "    # Check if the save directory exists, create if not\n",
    "    if not os.path.exists(save_path):\n",
    "        os.makedirs(save_path)\n",
    "\n",
    "    plt.style.use('seaborn-darkgrid')  # Enhanced style\n",
    "    plt.figure(figsize=FIG_SIZE)\n",
    "    plt.rcParams.update({'font.size': FONT_SIZE})\n",
    "\n",
    "    num_values = len(parameters)\n",
    "    for i, (parameter_value, runs_results) in enumerate(zip(parameters, range_of_results)):\n",
    "        means = torch.tensor(runs_results).float().mean(0)\n",
    "        plt.plot(EPISODE_RANGE, means, label=str(parameter_value), linewidth=LINE_WIDTH,\n",
    "                 color=COLOR_PALETTE(i / num_values))  # Apply color from palette\n",
    "\n",
    "    plt.ylabel(\"Average Return\", fontweight='bold')\n",
    "    plt.xlabel(\"Episode\", fontweight='bold')\n",
    "    plt.title(title, fontweight='bold')\n",
    "    plt.axhline(y=BLUE_LINE_Y, color=BLUE_LINE_COLOR, linestyle=BLUE_LINE_STYLE, linewidth=LINE_WIDTH)\n",
    "    plt.legend(loc=LEGEND_LOC)\n",
    "    plt.grid(True, linestyle=GRID_STYLE)\n",
    "\n",
    "    # Save the figure\n",
    "    plt.savefig(os.path.join(save_path, title.replace(\" \", \"_\") + '.png'))\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Hyperparemeters used when testing other parameters in isolation.\n",
    "hyperparams = {\n",
    "    \"lr\": 0.001,\n",
    "    \"e\": 0.8,\n",
    "    \"memory_replay_buffer_size\": 20000,\n",
    "    \"e_decay\": 0.99,\n",
    "    \"optimiser\": 'Adam',\n",
    "    \"act_func\": F.leaky_relu,\n",
    "    \"policy_nn\": [4, 128, 64, 2],\n",
    "    \"batch_space\": 20,\n",
    "    \"update_target_iteration\": 1,\n",
    "    \"episodes\": NUM_EPISODES\n",
    "}\n",
    "\n",
    "\n",
    "e_range = [0.1,0.3,0.5,0.7,0.9]\n",
    "\n",
    "e_decay_range = [0,0.9,0.99,1]\n",
    "\n",
    "activation_functions = [F.tanh, F.relu, F.leaky_relu, F.sigmoid]\n",
    "\n",
    "batch_range = [1,5,10,20,32]\n",
    "\n",
    "buffer_range = [100,500,1000,5000,10000,20000]\n",
    "\n",
    "lr_range = [0.001,0.01,0.1,0.5]\n",
    "\n",
    "optimiser_range = ['SGD', 'Adam', 'RAdam', 'RMSprop']\n",
    "\n",
    "policy_neural_networks = [\n",
    "        [4, 16, 2], \n",
    "        [4, 32, 2], \n",
    "        [4, 128, 2], \n",
    "        [4, 32, 16, 2], \n",
    "        [4, 128, 64, 2],  \n",
    "        [4, 64, 32, 16, 2]]\n",
    "\n",
    "iteration_range = [1,2,3,4,5,10,20]\n",
    "\n",
    "\n",
    "epsilon_results = parameter_testing(hyperparams, 'e', e_range)\n",
    "param_plot(epsilon_results, e_range, \"Epsilon Parameters Values\")\n",
    "\n",
    "exploration_results = parameter_testing(hyperparams, 'e', e_range)\n",
    "param_plot(exploration_results, e_range, \"Epsilon Parameters Values\")\n",
    "\n",
    "e_decay_results = parameter_testing(hyperparams, 'e_decay', e_decay_range)\n",
    "param_plot(e_decay_results, e_decay_range, \"Epsilon Decay Parameter Values\")\n",
    "\n",
    "activation_func_results = parameter_testing(hyperparams, 'act_func', activation_functions)\n",
    "param_plot(activation_func_results, [f.__name__ for f in activation_functions], \"Activation Function Parameter Values\")\n",
    "\n",
    "batch_results = parameter_testing(hyperparams, 'batch_space', batch_range)\n",
    "param_plot(batch_results, batch_range, \"Batch Size Parameter Values\")\n",
    "\n",
    "buffer_results = parameter_testing(hyperparams, 'memory_replay_buffer_size', buffer_range)\n",
    "param_plot(buffer_results, buffer_range, \"Memory Replay Buffer Size Parameter Values\")\n",
    "\n",
    "lr_results = parameter_testing(hyperparams, 'lr', lr_range)\n",
    "param_plot(lr_results, lr_range, \"Learning Rate Parameter Values\")\n",
    "\n",
    "optimiser_results = parameter_testing(hyperparams, 'optimiser', optimiser_range)\n",
    "param_plot(optimiser_results, optimiser_range, \"Optimizer Type Parameter Values\")\n",
    "\n",
    "policy_nn_results = parameter_testing(hyperparams, 'policy_nn', policy_neural_networks)\n",
    "param_plot(policy_nn_results, [str(nn) for nn in policy_neural_networks], \"Policy Neural Network Structure Parameter Values\")\n",
    "\n",
    "iteration_results = parameter_testing(hyperparams, 'update_target_iteration', iteration_range)\n",
    "param_plot(iteration_results, iteration_range, \"Update Target Iteration Parameter Values\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define hyperparameters as individual variables\n",
    "e = 0.8\n",
    "e_decay = 0.99\n",
    "lr = 0.001\n",
    "optimiser = 'Adam'\n",
    "policy_nn = [4, 128, 64, 2]\n",
    "act_func = F.leaky_relu\n",
    "batch_space = 10\n",
    "memory_replay_buffer_size = 2000\n",
    "update_target_iteration = 1\n",
    "episodes = 300\n",
    "\n",
    "# Execute the training\n",
    "learning_curve = run_dqn(\n",
    "    e, e_decay, lr, optimiser, policy_nn, act_func,\n",
    "    batch_space, memory_replay_buffer_size, update_target_iteration, episodes\n",
    ")\n",
    "\n",
    "\n",
    "# Define hyperparameters as individual variables\n",
    "e = 1\n",
    "e_decay = 1\n",
    "lr = 1\n",
    "optimiser = 'Adam'\n",
    "policy_nn = [4,2]\n",
    "act_func = F.relu\n",
    "batch_space = 1\n",
    "memory_replay_buffer_size = 1\n",
    "update_target_iteration = 1\n",
    "episodes = 300\n",
    "\n",
    "# Execute the training\n",
    "original = run_dqn(\n",
    "    e, e_decay, lr, optimiser, policy_nn, act_func,\n",
    "    batch_space, memory_replay_buffer_size, update_target_iteration, episodes\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 10))\n",
    "plt.rcParams.update({'font.size': 22, 'font.family': 'serif'})\n",
    "\n",
    "res = torch.tensor(original)\n",
    "m = res.float().mean(0)\n",
    "s = res.float().std(0)\n",
    "\n",
    "returns = torch.tensor(learning_curve)\n",
    "means = returns.float().mean(0)\n",
    "std = returns.float().std(0)\n",
    "\n",
    "plt.plot(torch.arange(300), m, label=\"Original hyperparameters\", linestyle='--', color='green')\n",
    "plt.fill_between(np.arange(300), m, m + s, alpha=0.5, color='green', hatch='.')\n",
    "plt.fill_between(np.arange(300), m, m - s, alpha=0.5, color='green', hatch='.')\n",
    "\n",
    "plt.plot(torch.arange(300), means, label=\"Our final tuned hyperparameters\", color='purple')\n",
    "plt.fill_between(np.arange(300), means, means + std, alpha=0.5, color='purple', hatch='\\\\')\n",
    "plt.fill_between(np.arange(300), means, means - std, alpha=0.5, color='purple', hatch='\\\\')\n",
    "\n",
    "plt.ylabel(\"Return\")\n",
    "plt.xlabel(\"Episode\")\n",
    "plt.title(\"Learning Curve of Model with Optimised Parameters\")\n",
    "plt.axhline(y=100, color='r', linestyle='--')\n",
    "plt.legend(loc='upper left', frameon=True, framealpha=0.7)\n",
    "\n",
    "plt.grid(True, linestyle='--', alpha=0.7)\n",
    "plt.gca().set_facecolor('#f2f2f2')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|This function selects an optimizer for the neural network based on the specified type.\n",
    "# Parameters:\n",
    "# - net: The neural network for which the optimizer is being chosen.\n",
    "# - lr_rate: Learning rate for the optimizer.\n",
    "# - opt_type: The type of optimizer to use. Options are 'Adam', 'RAdam', 'SGD', and 'RMSprop'.\n",
    "# Returns: An instantiated optimizer object for the given neural network.\n",
    "def choose_optimizer(net, lr_rate, opt_type):\n",
    "    # Dictionary mapping optimizer type names to their corresponding PyTorch classes.\n",
    "    opt_choices = {\n",
    "        'Adam': optim.Adam,\n",
    "        'RAdam': optim.RAdam,\n",
    "        'SGD': optim.SGD,\n",
    "        'RMSprop': optim.RMSprop \n",
    "    }\n",
    "\n",
    "    # If the chosen optimizer type is in the available choices, create and return the optimizer.\n",
    "    if opt_type in opt_choices:\n",
    "        return opt_choices[opt_type](net.parameters(), lr=lr_rate)\n",
    "    else:\n",
    "        # If an invalid optimizer type is passed, raise an error.\n",
    "        raise ValueError(\"Invalid optimizer type\")\n",
    "\n",
    "# This function sets up the training environment using OpenAI Gym.\n",
    "# Returns: The created Gym environment instance, specifically the 'CartPole-v1' environment.\n",
    "def setup_env():\n",
    "    return gym.make('CartPole-v1')\n",
    "\n",
    "# This function starts a new round in the game environment.\n",
    "# Parameters:\n",
    "# - env: The Gym environment in which the round is to be started.\n",
    "# Returns: The initial state vector of the environment after reset.\n",
    "def begin_round(env):\n",
    "    # Reset the environment and get the initial observation.\n",
    "    obs, _ = env.reset()\n",
    "\n",
    "    # Convert the observation to a PyTorch tensor and return it as the state vector.\n",
    "    state_vec = torch.tensor(obs).float()\n",
    "    return state_vec\n",
    "\n",
    "# This function performs an action in the game environment based on the current state.\n",
    "# Parameters:\n",
    "# - env: The Gym environment in which the action is to be performed.\n",
    "# - net: The neural network used for decision making.\n",
    "# - present_state: The current state of the environment.\n",
    "# - memory_buffer: The memory buffer to store experiences for training.\n",
    "# - eps: Epsilon value for epsilon-greedy strategy in decision making.\n",
    "# Returns: The next state after the action, and flags indicating whether the round is complete or finished.\n",
    "def perform_action(env, net, present_state, memory_buffer, eps):\n",
    "    # Determine the action to take using epsilon-greedy strategy.\n",
    "    action_chosen = epsilon_greedy(eps, net, present_state)\n",
    "\n",
    "    # Perform the chosen action in the environment and get the new observation and other info.\n",
    "    obs, reward, complete, finished, _ = env.step(action_chosen)\n",
    "\n",
    "    # Convert reward and action chosen to PyTorch tensors.\n",
    "    reward_val = torch.tensor([reward])\n",
    "    action_val = torch.tensor([action_chosen])\n",
    "\n",
    "    # Convert the new observation to a tensor and reshape it as needed.\n",
    "    next_state = torch.tensor(obs).reshape(-1).float()\n",
    "\n",
    "    # Store the experience in the memory buffer.\n",
    "    memory_buffer.push([present_state, action_val, next_state, reward_val, torch.tensor([complete])])\n",
    "\n",
    "    # Return the next state and flags indicating round completion status.\n",
    "    return next_state, complete, finished\n",
    "\n",
    "# Process a batch for training\n",
    "def process_batch(net, target_net, opt, memory_buffer, size_batch):\n",
    "    \"\"\"\n",
    "    Processes a batch of data for training the network.\n",
    "\n",
    "    Args:\n",
    "    - net: The current neural network model being trained.\n",
    "    - target_net: The target network used for calculating the loss.\n",
    "    - opt: The optimizer used for updating the weights.\n",
    "    - memory_buffer: The replay buffer containing the experiences.\n",
    "    - size_batch: The size of the batch to be processed.\n",
    "\n",
    "    This function will sample a batch from the memory buffer and\n",
    "    perform a training step, including forward and backward propagation.\n",
    "    \"\"\"\n",
    "    # Check if enough samples are available in the memory buffer\n",
    "    if len(memory_buffer.buffer) < size_batch:\n",
    "        return\n",
    "\n",
    "    # Sample a batch of experiences from the memory buffer\n",
    "    batch = memory_buffer.sample(size_batch)\n",
    "    # Unpack the batch into respective components and stack them\n",
    "    states, actions, next_states, rewards, completion = (torch.stack(x) for x in zip(*batch))\n",
    "    # Calculate the loss\n",
    "    loss_val = loss(net, target_net, states, actions, rewards, next_states, completion)\n",
    "    # Reset gradients before backpropagation\n",
    "    opt.zero_grad()\n",
    "    # Backward pass to compute gradients\n",
    "    loss_val.backward()\n",
    "    # Update the weights\n",
    "    opt.step()\n",
    "\n",
    "\n",
    "# Conduct a single training session\n",
    "def conduct_training(env, net, target_net, opt, memory_buffer, episodes_total, size_batch, eps, decay_eps, freq_update_target):\n",
    "    \"\"\"\n",
    "    Conducts a training session over a given number of episodes.\n",
    "\n",
    "    Args:\n",
    "    - env: The environment for the agent to interact with.\n",
    "    - net: The current neural network model.\n",
    "    - target_net: The target network for stable Q-value estimation.\n",
    "    - opt: The optimizer for updating the network weights.\n",
    "    - memory_buffer: The replay buffer for storing experiences.\n",
    "    - episodes_total: Total number of episodes for training.\n",
    "    - size_batch: Batch size for training.\n",
    "    - eps: Epsilon value for epsilon-greedy policy.\n",
    "    - decay_eps: Decay rate for epsilon.\n",
    "    - freq_update_target: Frequency of updating the target network.\n",
    "\n",
    "    Returns:\n",
    "    - lengths_episodes: A list containing the lengths of each episode.\n",
    "\n",
    "    This function conducts training over a specified number of episodes,\n",
    "    updating the network weights and target network at specified intervals.\n",
    "    \"\"\"\n",
    "    lengths_episodes = []\n",
    "\n",
    "    for ep in range(episodes_total):\n",
    "        present_state = begin_round(env)\n",
    "        complete = False\n",
    "        finished = False\n",
    "        count_steps = 0\n",
    "\n",
    "        while not (complete or finished):\n",
    "            # Perform an action based on the current state\n",
    "            present_state, complete, finished = perform_action(env, net, present_state, memory_buffer, eps)\n",
    "            # Process the batch for training\n",
    "            process_batch(net, target_net, opt, memory_buffer, size_batch)\n",
    "            \n",
    "            # Log episode length\n",
    "            if complete or finished:\n",
    "                lengths_episodes.append(count_steps + 1)\n",
    "            count_steps += 1\n",
    "\n",
    "            # Update epsilon using decay\n",
    "            eps = max(0.01, eps * decay_eps)\n",
    "\n",
    "        # Update target network at specified intervals\n",
    "        if ep % freq_update_target == 0: \n",
    "            update_target(target_net, net)\n",
    "\n",
    "    return lengths_episodes\n",
    "\n",
    "\n",
    "# Implement the DQN strategy\n",
    "def implement_dqn_strategy(eps, decay_eps, rate_lr, opt_type, arch_net, func_activation, size_batch, size_memory, freq_update, episodes_total, count_runs=1):\n",
    "    \"\"\"\n",
    "    Implements the Deep Q-Network (DQN) strategy for a given environment.\n",
    "\n",
    "    Args:\n",
    "    - eps: Starting value of epsilon for the epsilon-greedy policy.\n",
    "    - decay_eps: Decay rate for epsilon.\n",
    "    - rate_lr: Learning rate for the optimizer.\n",
    "    - opt_type: Type of optimizer to use.\n",
    "    - decay_wt: Weight decay for the optimizer.\n",
    "    - arch_net: Architecture of the neural network.\n",
    "    - func_activation: Activation function for the neural network.\n",
    "    - size_batch: Batch size for training.\n",
    "    - size_memory: Size of the replay buffer.\n",
    "    - freq_update: Frequency of updating the target network.\n",
    "    - episodes_total: Total number of training episodes.\n",
    "    - count_runs: Number of runs to perform (default is 1).\n",
    "\n",
    "    Returns:\n",
    "    - net: The trained neural network model.\n",
    "\n",
    "    This function sets up the environment and DQN network, then conducts\n",
    "    training over a number of runs, returning the trained network.\n",
    "    \"\"\"\n",
    "    outcomes = []\n",
    "    env = setup_env()\n",
    "\n",
    "    for _ in range(count_runs):\n",
    "        # Initialize the DQN network and the target network\n",
    "        net = DQN(arch_net, func_activation)\n",
    "        target_net = DQN(arch_net, func_activation)\n",
    "        update_target(target_net, net)\n",
    "        target_net.eval()\n",
    "\n",
    "        # Choose the optimizer based on input parameters\n",
    "        opt = choose_optimizer(net, rate_lr, opt_type)\n",
    "\n",
    "        # Initialize the replay buffer\n",
    "        memory_buffer = ReplayBuffer(size_memory)\n",
    "\n",
    "        # Conduct the training session\n",
    "        durations_episodes = conduct_training(\n",
    "            env, net, target_net, opt, memory_buffer, episodes_total, size_batch, eps, decay_eps, freq_update\n",
    "        )\n",
    "        outcomes.append(durations_episodes)\n",
    "\n",
    "    return net\n",
    "\n",
    "\n",
    "# Model configuration\n",
    "model_config = {\n",
    "    \"eps\": 0.8,\n",
    "    \"decay_eps\": 0.99,\n",
    "    \"rate_lr\": 0.001,\n",
    "    \"opt_type\": 'Adam',\n",
    "    \"arch_net\": [4, 128, 64, 2],\n",
    "    \"func_activation\": F.leaky_relu,\n",
    "    \"size_batch\": 10,\n",
    "    \"size_memory\": 20000,\n",
    "    \"freq_update\": 1,\n",
    "    \"episodes_total\": 300,\n",
    "    \"count_runs\": 10\n",
    "}\n",
    "\n",
    "# Execute the refactored function\n",
    "trained_dqn = implement_dqn_strategy(**model_config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(dqn_policy, cart_velocity, compute_value):\n",
    "    \"\"\"\n",
    "    Prepares the data for plotting by computing the values based on the provided function 'compute_value'.\n",
    "    \"\"\"\n",
    "    max_cart_angle = .2095\n",
    "    max_angular_velocity = 2\n",
    "\n",
    "    num_angle_samples = 100\n",
    "    num_omega_samples = 100\n",
    "    cart_angles = torch.linspace(max_cart_angle, -max_cart_angle, num_angle_samples)\n",
    "    angular_velocities = torch.linspace(-max_angular_velocity, max_angular_velocity, num_omega_samples)\n",
    "\n",
    "    value_matrix = torch.zeros((num_angle_samples, num_omega_samples))\n",
    "    for angle_idx, cart_angle in enumerate(cart_angles):\n",
    "        for omega_idx, angular_velocity in enumerate(angular_velocities):\n",
    "            cart_state = torch.tensor([0., cart_velocity, cart_angle, angular_velocity])\n",
    "            with torch.no_grad():\n",
    "                action_values = dqn_policy(cart_state)\n",
    "                value_matrix[angle_idx, omega_idx] = compute_value(action_values)\n",
    "    \n",
    "    return cart_angles, angular_velocities, value_matrix\n",
    "\n",
    "def plot_matrix(cart_angles, angular_velocities, value_matrix, plot_title, cmap, colorbar=False, save_path=''):\n",
    "    \"\"\"\n",
    "    Plots the provided matrix using matplotlib and saves it to a file.\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(10,8))\n",
    "    plt.rcParams.update({'font.size': 14})\n",
    "\n",
    "    plt.contourf(cart_angles, angular_velocities, value_matrix.T, cmap=cmap, levels=100 if colorbar else None)\n",
    "    if colorbar:\n",
    "        cbar = plt.colorbar()\n",
    "        cbar.set_label('Q-value', rotation=0, labelpad=15)  # Label for the color bar\n",
    "    if not colorbar:\n",
    "        # Updated legend descriptions\n",
    "        right_action_patch = plt.matplotlib.patches.Patch(color='yellow', label='Push cart to right')\n",
    "        left_action_patch = plt.matplotlib.patches.Patch(color='blue', label='Push cart to left')\n",
    "        plt.legend(handles=[right_action_patch, left_action_patch])\n",
    "    \n",
    "    plt.xlabel(\"Angle (rad)\")\n",
    "    plt.ylabel(\"Angular Velocity (rad/s)\")\n",
    "    plt.title(plot_title)\n",
    "\n",
    "    if save_path:\n",
    "        if not os.path.exists('images'):\n",
    "            os.makedirs('images')\n",
    "        plt.savefig(f'images/{save_path}.png', bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "def plot_policy_actions(dqn_policy, cart_velocity, plot_title, file_name):\n",
    "    cart_angles, angular_velocities, action_decision_matrix = prepare_data(\n",
    "        dqn_policy, cart_velocity, lambda values: values.argmax()\n",
    "    )\n",
    "    plot_matrix(cart_angles, angular_velocities, action_decision_matrix, plot_title, cmap='cividis', save_path=file_name)\n",
    "\n",
    "def plot_q_value_visualization(dqn_policy, cart_velocity, plot_title, file_name):\n",
    "    cart_angles, angular_velocities, q_value_matrix = prepare_data(\n",
    "        dqn_policy, cart_velocity, lambda values: values.max()\n",
    "    )\n",
    "    plot_matrix(cart_angles, angular_velocities, q_value_matrix, plot_title, cmap='cividis', colorbar=True, save_path=file_name)\n",
    "\n",
    "# trained_dqn = runs_results\n",
    "\n",
    "# Policy Actions Plots\n",
    "plot_policy_actions(trained_dqn, cart_velocity=0, plot_title=\"Greedy Policy: Position 0, Velocity 0\", file_name=\"policy_action_0_0\")\n",
    "plot_policy_actions(trained_dqn, cart_velocity=0.5, plot_title=\"Greedy Policy: Position 0, Velocity 0.5\", file_name=\"policy_action_0_0.5\")\n",
    "plot_policy_actions(trained_dqn, cart_velocity=1, plot_title=\"Greedy Policy: Position 0, Velocity 1\", file_name=\"policy_action_0_1\")\n",
    "plot_policy_actions(trained_dqn, cart_velocity=2, plot_title=\"Greedy Policy: Position 0, Velocity 2\", file_name=\"policy_action_0_2\")\n",
    "\n",
    "# Q Value Visualization Plots\n",
    "plot_q_value_visualization(trained_dqn, cart_velocity=0, plot_title=\"Q Position: Position 0, Velocity 0\", file_name=\"q_value_vis_0_0\")\n",
    "plot_q_value_visualization(trained_dqn, cart_velocity=0.5, plot_title=\"Q Position: Position 0, Velocity 0.5\", file_name=\"q_value_vis_0_0.5\")\n",
    "plot_q_value_visualization(trained_dqn, cart_velocity=1, plot_title=\"Q Position: Position 0, Velocity 1\", file_name=\"q_value_vis_0_1\")\n",
    "plot_q_value_visualization(trained_dqn, cart_velocity=2, plot_title=\"Q Position: Position 0, Velocity 2\", file_name=\"q_value_vis_0_2\")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "0a913fe633d275fcd93cd74d17a0efaea0a3859e603406f44895a745403f4397"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit ('rl-env': venv)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "19a620d5ac44fc7d0064c056f791ee6a0d71d7061288207e2ea49537a4625f93"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
